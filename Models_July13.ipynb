{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b3faa5b-fc05-4b9f-ba47-b46767b5529e",
   "metadata": {},
   "source": [
    "### Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77d3532f-6fef-4245-a182-6cac6381a33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667d0cea-331a-407d-a097-e1519cd4a1f7",
   "metadata": {},
   "source": [
    "### Read and clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14bbd591-eae9-4f6f-bfa1-f8e3fa43489f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data2021 = pd.read_csv('2021_RAW_APC_Data.csv')\n",
    "data2019 = pd.read_csv('2019_RAW_APC_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c820e7f-e1b7-4365-8e28-71098c283f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['route finish time','route start time','stop arrival time']:\n",
    "    data2019[col] = pd.to_datetime(data2019[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ea1815-91c0-4589-88b0-994e16c1461c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf6b07a3-b478-4a02-b914-6d9ce2b56341",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data2019\n",
    "\n",
    "df['Crowded'] = df['passwithin']>74\n",
    "df['Supercrowded'] = df['passwithin']>134\n",
    "df['Capacity'] = df['passwithin']>194\n",
    "df['UnderNeg5'] = df['passwithin']<-5\n",
    "df['NegAFew'] = df['passwithin'].between(-5,-1)\n",
    "df['Over250'] = df['passwithin']>250\n",
    "\n",
    "df['Crowded000'] = df['passwithin']>0\n",
    "df['Crowded010'] = df['passwithin']>10\n",
    "df['Crowded020'] = df['passwithin']>20\n",
    "df['Crowded030'] = df['passwithin']>30\n",
    "df['Crowded040'] = df['passwithin']>40\n",
    "df['Crowded050'] = df['passwithin']>50\n",
    "df['Crowded060'] = df['passwithin']>60\n",
    "df['Crowded070'] = df['passwithin']>70\n",
    "df['Crowded080'] = df['passwithin']>80\n",
    "df['Crowded090'] = df['passwithin']>90\n",
    "df['Crowded100'] = df['passwithin']>100\n",
    "df['Crowded110'] = df['passwithin']>110\n",
    "df['Crowded120'] = df['passwithin']>120\n",
    "df['Crowded130'] = df['passwithin']>130\n",
    "df['Crowded140'] = df['passwithin']>140\n",
    "df['Crowded150'] = df['passwithin']>150\n",
    "df['Crowded160'] = df['passwithin']>160\n",
    "df['Crowded170'] = df['passwithin']>170\n",
    "df['Crowded180'] = df['passwithin']>180\n",
    "df['Crowded190'] = df['passwithin']>190\n",
    "df['Crowded200'] = df['passwithin']>200\n",
    "df['Crowded210'] = df['passwithin']>210\n",
    "df['Crowded220'] = df['passwithin']>220\n",
    "df['Crowded230'] = df['passwithin']>230\n",
    "df['Crowded240'] = df['passwithin']>240"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6a01b37-fc73-45c4-b8f5-b7f00690cc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data2019\n",
    "\n",
    "df['TOD'] = df['stop arrival time'].dt.time\n",
    "df['DOW'] = df['stop arrival time'].dt.dayofweek # 0 is Monday, 6 is Sunday\n",
    "df['DOW_name'] = df['stop arrival time'].dt.day_name()\n",
    "df['Date'] = df['stop arrival time'].dt.date\n",
    "df['Hour'] = df['stop arrival time'].dt.hour\n",
    "df['Minute'] = df['stop arrival time'].dt.minute\n",
    "df['Minute_od'] = df['Hour'] * 60 + df['Minute']\n",
    "df['Month'] = df['stop arrival time'].dt.month\n",
    "df['Month_name'] = df['stop arrival time'].dt.month_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92246314-ffba-49a9-a3fa-2134996db894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, just see how many rows are in each routedone\n",
    "rtd = data2019.groupby('routedone').count()['railcar ID']\n",
    "\n",
    "rtd.name = 'count'\n",
    "\n",
    "overmuch = rtd[rtd>20]\n",
    "\n",
    "df01 = data2019[~data2019['routedone'].isin(overmuch.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "926e1cef-e591-4fd2-992c-bd59506e34cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df02 is df01 but without >210 observations\n",
    "df02 = df01[~df01['Crowded210']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c659e513-1be7-404b-a19d-264df66b51db",
   "metadata": {},
   "source": [
    "Work with df02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b650e7-5070-4694-96b4-aa5653754eba",
   "metadata": {},
   "source": [
    "Add DOW, TOD, Time of year, and station"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5013a2-b5cd-4345-9857-aebca8796aec",
   "metadata": {},
   "source": [
    "Create ML dataset\n",
    "\n",
    "Need to create test/train sets**\n",
    "\n",
    "Readout: need precision and recall.\n",
    "\n",
    "Precision is TP / (TP+FP). Of all times we predicted the train was crowded, how often was the train crowded?\n",
    "\n",
    "Recall is TP / (TP+FN). Of all times the train was crowded, how often did we predict that the train was crowded?\n",
    "\n",
    "F1-score is a weighted average of the two.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c8038569-1779-4ae9-9c41-dabb4b5f012f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doesn't even take into account station\n",
    "feats01 = ['Minute_od','DOW','Month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "80847516-736e-456f-ac00-6069f6ad2d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df02\n",
    "feats = feats01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "24f44051-0b28-4253-8a48-6a92038df96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.loc[:,feats]\n",
    "\n",
    "y = df.loc[:,'Crowded'].astype(int)\n",
    "\n",
    "X, y = shuffle(X, y, random_state=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c902b73b-bd9e-46f1-a591-69ef04445d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "50a9d84b-2970-4ba0-a958-395dabc8c2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9baefefe-bbdf-481c-8bd2-56feb6c8e267",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "61826ada-aa25-44ba-8aa3-c6f18ae0b274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_readout(model, X, y):\n",
    "    print('Performing 5-fold cross-validation...')\n",
    "    scores = cross_validate(model, X, y, cv=5,\n",
    "                            scoring=['precision','recall','f1'])\n",
    "    \n",
    "    print(scores)\n",
    "    #print('The mean of the RMSEs is', -np.mean(scores['test_score']))\n",
    "    #print('The standard deviation of the RMSEs is', np.std(scores['test_score'])\n",
    "    \n",
    "          # could redo this in the future so that all models can be in one df\n",
    "          # for now, this df contains just one model's results\n",
    "    #cv_results = pd.DataFrame(columns=['Precision','Recall','F1'], data= [)         \n",
    "    #return cv_results\n",
    "    print(\n",
    "    np.mean(scores['test_precision']),\n",
    "    np.mean(scores['test_recall']),\n",
    "    np.mean(scores['test_f1']),\n",
    "    np.std(scores['test_precision']),\n",
    "    np.std(scores['test_recall']),\n",
    "    np.std(scores['test_f1'])\n",
    "    )\n",
    "    \n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b88ce0-f8ae-4d30-b23f-ed6ca8a59b0c",
   "metadata": {},
   "source": [
    "## Can I compare with Google's data? statistically?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3b417a91-661f-48e5-a62a-9e80f5f76471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126670\n",
      "1295022\n"
     ]
    }
   ],
   "source": [
    "print(sum(y))\n",
    "\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43e2827-e7dc-4649-9913-c121a917d9a1",
   "metadata": {},
   "source": [
    "## Hello\n",
    "\n",
    "## Notes 7/15\n",
    "\n",
    "Can I compare with Google's data statistically?\n",
    "\n",
    "Note: this dataset is too large. Don't try to fit a Random Forest on the entire dataset (at least this is true on my old laptop!)\n",
    "\n",
    "Make a scatterplot/pairplot - Day of week and time of day and month - see whether it looks seperable (can we separate Crowded from Non-Crowded based on these variables)\n",
    "\n",
    "Class imbalance\n",
    "\n",
    "Cluster in a compelling way\n",
    "\n",
    "Send PB my visualizations\n",
    "\n",
    "\n",
    "What makes a train arrival a train arrival\n",
    "\n",
    "PB: The basic unit is the hour-span at a station. (If there are 3 arrivals during that span, then we average the crowdedness of the 3 arrivals. If there is 1 arrival during that span, then we use the crowdedness of that 1.\n",
    "-- do 24 hour-spans per day, 365 days in the year = 8760 hour-spans. Then we have 16 stations so our data is 8760 x 16. So essentially we're using PCA to simplify 8760 dimensions down to much fewer. Cool. So we represent the *stations* in the PC-space. Clustering here will tell us if there are essentially 2 types of stations or essentially 3 types of stations, etc. We now better understand types of stations.\n",
    "-- alternatively, we could seek to understand what types of days there are. We represent each of the 365 days in the year by 24 features per station. Becuase 24 X 16 stations = 384, we're using PCA to simplify 384 dimensions down to much fewer. We represent *days* in the PC-space, and clustering tells us essentially what kinds of days exist.\n",
    "\n",
    "(In both of the above, the PCs are shorthand for different crowding patterns)\n",
    "\n",
    "AP: The basic unit is the train arrival. We want to understand what types of train arrivals there are. So, we cluster in the PC-space. But how did we get these Principal components?\n",
    "-- do some principal components regarding crowding patterns (using nbr boarding, nbr deboarding, nbr within) at the last few stops of this train\n",
    "---- this should be a different PC-Analysis for each stop\n",
    "---- clustering then ansswers the question \"What kinds of train conditionss are there?\n",
    "-- Do PCs that capture TOD, DOW, \n",
    "\n",
    "Visualize in a way that says what is the appropriate agg strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "feb4fa8a-f82f-4932-9476-973b0e085160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing 5-fold cross-validation...\n",
      "{'fit_time': array([0.43317032, 0.43978477, 0.43702817, 0.43745327, 0.44233346]), 'score_time': array([0.04686999, 0.04863167, 0.04686022, 0.03124833, 0.03125024]), 'test_precision': array([0.25477707, 0.22142857, 0.25503356, 0.25827815, 0.26153846]), 'test_recall': array([0.20512821, 0.15897436, 0.19487179, 0.19897959, 0.17346939]), 'test_f1': array([0.22727273, 0.18507463, 0.22093023, 0.22478386, 0.20858896])}\n",
      "0.2502111611546143 0.18628466771323915 0.21333008108464457 0.014600852517665082 0.017320872480715992 0.01551900592860361\n"
     ]
    }
   ],
   "source": [
    "rf01 = RandomForestClassifier()\n",
    "scores = cv_readout(rf01,\n",
    "                    X_train.sample(10000, random_state=19),\n",
    "                    y_train.sample(10000, random_state=19))\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d7f3b52e-226b-429c-a562-740be67fc646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing 5-fold cross-validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\August\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\August\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\August\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\August\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([0.0376544 , 0.04687333, 0.04687047, 0.06065869, 0.05397844]), 'score_time': array([0.01562381, 0.        , 0.01562428, 0.00899768, 0.015625  ]), 'test_precision': array([0., 0., 0., 0., 0.]), 'test_recall': array([0., 0., 0., 0., 0.]), 'test_f1': array([0., 0., 0., 0., 0.])}\n",
      "0.0 0.0 0.0 0.0 0.0 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\August\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "logi01 = LogisticRegression()\n",
    "scores = cv_readout(logi01,\n",
    "                    X_train.sample(10000, random_state=19),\n",
    "                    y_train.sample(10000, random_state=19))\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7c050ee7-5748-4188-a801-5313a7461a69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['accuracy',\n",
       " 'adjusted_mutual_info_score',\n",
       " 'adjusted_rand_score',\n",
       " 'average_precision',\n",
       " 'balanced_accuracy',\n",
       " 'completeness_score',\n",
       " 'explained_variance',\n",
       " 'f1',\n",
       " 'f1_macro',\n",
       " 'f1_micro',\n",
       " 'f1_samples',\n",
       " 'f1_weighted',\n",
       " 'fowlkes_mallows_score',\n",
       " 'homogeneity_score',\n",
       " 'jaccard',\n",
       " 'jaccard_macro',\n",
       " 'jaccard_micro',\n",
       " 'jaccard_samples',\n",
       " 'jaccard_weighted',\n",
       " 'max_error',\n",
       " 'mutual_info_score',\n",
       " 'neg_brier_score',\n",
       " 'neg_log_loss',\n",
       " 'neg_mean_absolute_error',\n",
       " 'neg_mean_absolute_percentage_error',\n",
       " 'neg_mean_gamma_deviance',\n",
       " 'neg_mean_poisson_deviance',\n",
       " 'neg_mean_squared_error',\n",
       " 'neg_mean_squared_log_error',\n",
       " 'neg_median_absolute_error',\n",
       " 'neg_root_mean_squared_error',\n",
       " 'normalized_mutual_info_score',\n",
       " 'precision',\n",
       " 'precision_macro',\n",
       " 'precision_micro',\n",
       " 'precision_samples',\n",
       " 'precision_weighted',\n",
       " 'r2',\n",
       " 'rand_score',\n",
       " 'recall',\n",
       " 'recall_macro',\n",
       " 'recall_micro',\n",
       " 'recall_samples',\n",
       " 'recall_weighted',\n",
       " 'roc_auc',\n",
       " 'roc_auc_ovo',\n",
       " 'roc_auc_ovo_weighted',\n",
       " 'roc_auc_ovr',\n",
       " 'roc_auc_ovr_weighted',\n",
       " 'top_k_accuracy',\n",
       " 'v_measure_score']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import SCORERS\n",
    "\n",
    "sorted(SCORERS.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a10211-84d9-47a3-bc84-11d9f0c1ef10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04d95b7-efca-4be4-b868-24bbba88dc9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3624703c-7bd9-4924-8203-ddac8f912668",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fcca1a-94cb-45aa-bd5c-0a4e60c51a85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8220f2f4-3fe3-418e-b035-ebacd2626fd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afe49dc-6ea4-4b85-b16c-736e7383003e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1623437f-881c-4636-9e73-da166ca6a19f",
   "metadata": {},
   "source": [
    "*Is there a \"soft classification\" technique? I.e. something where we want to classify over 74 versus under 74, but we penalize farther away errors more?\n",
    "\n",
    "** that might be a regression problem with an interesting error function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd73082d-b2bd-4bd9-837a-9998bf175435",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c8524a-d137-4d5a-8985-2511bb1e9c9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd93967-8194-4d32-b0bf-7dfc7d041096",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb7e831-0b38-48f1-a988-98211931c94a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eccd0c1-c80c-4b08-9830-7256e670c1e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41323b14-4a99-43fb-a403-df3fcea1acce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e181ba4-6f51-47ea-948b-4a5842dbfb43",
   "metadata": {},
   "source": [
    "### Word bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c433412c-804c-4c36-ae74-07570e5a9b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# supervised machine learning\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate\n",
    "import statsmodels.api as sm\n",
    "import warnings \n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.svm import SVR"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
